import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import requests
from wordcloud import WordCloud
import json
import seaborn as sns

# Title and Introduction
st.title("AI-Powered Threat Hunting and Vulnerability Assessment Dashboard")
st.image("logo.webp", width=500) 
st.write("This app provides real-time threat detection, vulnerability scanning, and log analysis.")

# Tabs for functionality
tabs = st.tabs(["Threat Detection", "Vulnerability Scanning", "Log Analysis"])

# --- Threat Detection ---
with tabs[0]:
    st.header("Threat Detection")

    # File upload for network data
    file = st.file_uploader("Upload Network Traffic Data (CSV)", type="csv")

    if file:
        # Load the dataset
        df = pd.read_csv(file)

        # Check and display data columns
        st.write("Preview of Uploaded Data:")
        st.dataframe(df.head())

        # Ensure the necessary numeric columns are present for anomaly detection
        if 'packet_size' not in df.columns or 'timestamp' not in df.columns:
            st.error("Missing necessary columns for anomaly detection. Please ensure 'packet_size' and 'timestamp' are present.")
        else:
            # Preprocessing and scaling numeric data
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            st.write(f"Using columns for anomaly detection: {', '.join(numeric_columns)}")

            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(df[numeric_columns])

            # Anomaly detection using Isolation Forest
            model = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)
            df['Anomaly'] = model.fit_predict(scaled_data)

            # Display anomalies
            st.write("Detected Anomalies:")
            st.dataframe(df[df['Anomaly'] == -1])

            # Visualization of anomalies
            st.write("Anomaly Visualization (Based on Packet Size):")
            sns.scatterplot(x=df.index, y=df['packet_size'], hue=df['Anomaly'], palette='coolwarm')
            plt.title("Anomaly Detection based on Packet Size")
            plt.xlabel("Index")
            plt.ylabel("Packet Size")
            st.pyplot(plt)

            # Optional: Anomaly count statistics
            anomaly_count = df['Anomaly'].value_counts()
            st.write(f"Anomaly Count: {anomaly_count[-1]} detected anomalies")
            st.write(f"Normal Count: {anomaly_count[1]} normal instances")
            
            # Optional: Show the full dataframe with anomaly results
            show_full = st.checkbox("Show full dataset with anomaly labels")
            if show_full:
                st.dataframe(df)

# --- Vulnerability Scanning ---
with tabs[1]:
    st.header("Vulnerability Scanning")

    # Load vulnerabilities from JSON file
    try:
        with open('vulnerabilities.json', 'r') as file:
            vulnerabilities = json.load(file)

        # Display vulnerabilities
        if vulnerabilities:
            for vuln in vulnerabilities:
                st.subheader(vuln['id'])
                st.write(f"**Summary**: {vuln['summary']}")
                st.write(f"**Published Date**: {vuln['published_date']}")
                st.write(f"**Severity**: {vuln['severity']}")
                st.write("---")
        else:
            st.write("No vulnerabilities found in the JSON file.")
    except Exception as e:
        st.error(f"Error loading vulnerabilities: {e}")

# --- Log Analysis ---
with tabs[2]:
    st.header("Log Analysis")

    # Log file upload
    log_file = st.file_uploader("Upload Log File (Text/CSV)", type=["txt", "csv"])

    if log_file:
        logs = log_file.read().decode("utf-8")
        st.write("Uploaded Log File Content:")
        st.text_area("Logs:", logs, height=300)

        # Basic analysis (example: count failed logins)
        if "failed" in logs.lower():
            st.warning("Failed login attempts detected!")
        else:
            st.success("No failed logins detected.")

        # Visualization
        st.write("Log File Word Cloud:")
        wordcloud = WordCloud(background_color="white").generate(logs)
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        st.pyplot(plt)

    # Add log data generation functionality
    st.subheader("Generate Advanced Log Data")
    if st.button("Generate Log Data CSV"):
        # Data for the advanced log
        data = [
            ['2025-01-23 10:00:00', 'INFO', 'System initialized', 'N/A', '192.168.1.1', 'N/A', 'Started service'],
            ['2025-01-23 10:01:00', 'WARNING', 'Potential issue detected', 'admin', '192.168.1.1', 'N/A', 'Checked system health'],
            ['2025-01-23 10:02:00', 'ERROR', 'Failed login attempt', 'guest', '192.168.1.10', '401', 'Attempted login'],
            ['2025-01-23 10:03:00', 'INFO', 'User login successful', 'admin', '192.168.1.2', 'N/A', 'Logged in'],
            ['2025-01-23 10:04:00', 'ERROR', 'Access denied for resource', 'admin', '192.168.1.3', '403', 'Accessing restricted page'],
            ['2025-01-23 10:05:00', 'INFO', 'File upload completed', 'admin', '192.168.1.4', 'N/A', 'Uploaded file'],
            ['2025-01-23 10:06:00', 'ERROR', 'Database connection failed', 'admin', '192.168.1.5', '500', 'Attempted connection'],
            ['2025-01-23 10:07:00', 'INFO', 'System maintenance started', 'N/A', '192.168.1.1', 'N/A', 'Performed maintenance'],
            ['2025-01-23 10:08:00', 'WARNING', 'High memory usage detected', 'monitoring', '192.168.1.6', 'N/A', 'Monitored system'],
            ['2025-01-23 10:09:00', 'INFO', 'Service restarted', 'admin', '192.168.1.7', 'N/A', 'Restarted service']
        ]

        # Create DataFrame
        columns = ['timestamp', 'log_level', 'message', 'user_id', 'ip_address', 'error_code', 'action']
        df = pd.DataFrame(data, columns=columns)

        # Save to CSV
        df.to_csv('advanced_log_data.csv', index=False)
        st.success("CSV file 'advanced_log_data.csv' has been created!")

# Footer
st.write("---")
st.markdown("**Developed by Your Gaurav Ghandat**")

